{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d414707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "alltag = pd.read_csv(\"C:/Users/arisa/StackOverflow-Project/dataset/all_tags.csv\")\n",
    "\n",
    "def createRequest(url):\n",
    "\n",
    "    params = {\n",
    "        'filter': '!T1gn2_YmiRkEHaO(cX',\n",
    "        'key': '8hvLA7uVPEIp6wwaqNOF5w((',\n",
    "        'pagesize': '20',\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.request(\n",
    "            \"GET\",\n",
    "            url,\n",
    "            params = params\n",
    "        )\n",
    "    except requests.exceptions.RequestException as e:  \n",
    "        print(\"ERROR: {}\".format(e))\n",
    "\n",
    "    json_data = json.loads(response.text)\n",
    "\n",
    "    return json_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    combined_tags = set()\n",
    "    for i in range(len(alltag[\"tags\"])):\n",
    "      dupall = [x.strip(\"\\''\") for x in alltag[\"tags\"][i].strip( '][' ).split( ', ' )]\n",
    "      for dup in dupall:\n",
    "        combined_tags.add(dup)\n",
    "\n",
    "    combined_tags = sorted(list(combined_tags))\n",
    "\n",
    "    for tag in combined_tags:\n",
    "        url = \"https://api.stackexchange.com/2.3/tags/{}/wikis?site=stackoverflow\".format(tag)\n",
    "            \n",
    "        res = createRequest(url)\n",
    "\n",
    "        json_file = \"C:/Users/arisa/StackOverflow-Project/duplicatetag/{}.json\".format(tag)\n",
    "        #json_file = tag + \".json\"\n",
    "        print(\"dump {} ...\".format(json_file))\n",
    "        with open(json_file, 'w') as json_f:\n",
    "            json.dump(res, json_f, indent=4)\n",
    "\n",
    "        time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c08ba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "alltag = pd.read_csv(\"C:/Users/arisa/StackOverflow-Project/dataset/all_tags.csv\")\n",
    "combined_tags = set()\n",
    "for i in range(len(alltag[\"tags\"])):\n",
    "    dupall = [x.strip(\"\\''\") for x in alltag[\"tags\"][i].strip( '][' ).split( ', ' )]\n",
    "    for dup in dupall:\n",
    "        combined_tags.add(dup)\n",
    "\n",
    "combined_tags = sorted(list(combined_tags))\n",
    "combined_tags.index(\"miui\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eee0f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from_index = \"2243\" # billing\n",
    "to_index = \"2244\" # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaac432",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "alltag = pd.read_csv(\"C:/Users/arisa/StackOverflow-Project/dataset/all_tags.csv\")\n",
    "\n",
    "def createRequest(url):\n",
    "\n",
    "    params = {\n",
    "        'filter': '!T1gn2_YmiRkEHaO(cX',\n",
    "        'key': '8hvLA7uVPEIp6wwaqNOF5w((',\n",
    "        'pagesize': '20',\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.request(\n",
    "            \"GET\",\n",
    "            url,\n",
    "            params = params\n",
    "        )\n",
    "    except requests.exceptions.RequestException as e:  \n",
    "        print(\"ERROR: {}\".format(e))\n",
    "\n",
    "    json_data = json.loads(response.text)\n",
    "\n",
    "    return json_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    combined_tags = set()\n",
    "    for i in range(len(alltag[\"tags\"])):\n",
    "      dupall = [x.strip(\"\\''\") for x in alltag[\"tags\"][i].strip( '][' ).split( ', ' )]\n",
    "      for dup in dupall:\n",
    "        combined_tags.add(dup)\n",
    "\n",
    "    combined_tags = sorted(list(combined_tags)) #if comnined_tags not in error_id\": 404\n",
    "    \n",
    "    start = 11933\n",
    "\n",
    "    for i in range(start, len(combined_tags)):\n",
    "        \n",
    "        tag = combined_tags[i]\n",
    "        \n",
    "        url = \"https://api.stackexchange.com/2.3/tags/{{{}}}/wikis?site=stackoverflow\".format(tag)\n",
    "            \n",
    "        res = createRequest(url)\n",
    "\n",
    "        json_file = \"C:/Users/arisa/StackOverflow-Project/duplicatetag/{}.json\".format(tag)\n",
    "        #json_file = tag + \".json\"\n",
    "        print(\"dump {} ...\".format(json_file))\n",
    "        with open(json_file, 'w') as json_f:\n",
    "            json.dump(res, json_f, indent=4)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d91a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_tags[11933]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d97733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_tags.index(\"c#-2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962a14fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_tags.index(\"c#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73a032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_tags.index(\"c#-7.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb70043",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_tags.index(\"zynq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9697b7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "c#-2.0.json\n",
    "removeduplicate\n",
    "yesterday\n",
    "c#-3.0.json\n",
    "removeduplicate\n",
    "yesterday\n",
    "c#-4.0.json\n",
    "removeduplicate\n",
    "yesterday\n",
    "c#-5.0.json\n",
    "removeduplicate\n",
    "yesterday\n",
    "c#-6.0.json\n",
    "removeduplicate\n",
    "yesterday\n",
    "c#-7.0.json\n",
    "removeduplicate\n",
    "yesterday\n",
    "c#-datatable.json\n",
    "removeduplicate\n",
    "yesterday\n",
    "c#-to-f#.json\n",
    "removeduplicate\n",
    "yesterday\n",
    "c#.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d728ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://api.stackexchange.com/2.3/tags/c%23/wikis\"\n",
    "\n",
    "params = {\n",
    "    'site': 'stackoverflow',\n",
    "    'key': '8hvLA7uVPEIp6wwaqNOF5w((',\n",
    "    'pagesize': '20',\n",
    "}\n",
    "\n",
    "response = requests.request(\n",
    "    \"GET\",\n",
    "    url,\n",
    "    params = params\n",
    "    \n",
    ")\n",
    "\n",
    "print(response.text)\n",
    "\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "json_file = \"C:/Users/arisa/StackOverflow-Project/duplicatetag/{}.json\".format(str(\"c#-to-f#\"))\n",
    "print(\"dump {} ...\".format(json_file))\n",
    "with open(json_file, 'w') as json_f:\n",
    "    json.dump(json_data, json_f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9616eaea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "alltag = pd.read_csv(\"C:/Users/arisa/StackOverflow-Project/dataset/all_tags.csv\")\n",
    "\n",
    "def createRequest(url):\n",
    "\n",
    "    params = {\n",
    "        'filter': '!T1gn2_YmiRkEHaO(cX',\n",
    "        'key': '8hvLA7uVPEIp6wwaqNOF5w((',\n",
    "        'pagesize': '20',\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.request(\n",
    "            \"GET\",\n",
    "            url,\n",
    "            params = params\n",
    "        )\n",
    "    except requests.exceptions.RequestException as e:  \n",
    "        print(\"ERROR: {}\".format(e))\n",
    "\n",
    "    json_data = json.loads(response.text)\n",
    "\n",
    "    return json_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    combined_tags = set()\n",
    "    for i in range(len(alltag[\"tags\"])):\n",
    "      dupall = [x.strip(\"\\''\") for x in alltag[\"tags\"][i].strip( '][' ).split( ', ' )]\n",
    "      for dup in dupall:\n",
    "        combined_tags.add(dup)\n",
    "\n",
    "    combined_tags = sorted(list(combined_tags)) #if comnined_tags not in error_id\": 404\n",
    "    \n",
    "    start = 22183\n",
    "\n",
    "    for i in range(start, 1000+start):\n",
    "        \n",
    "        tag = combined_tags[i]\n",
    "        \n",
    "        url = \"https://api.stackexchange.com/2.3/tags/{{{}}}/wikis?site=stackoverflow\".format(tag)\n",
    "            \n",
    "        res = createRequest(url)\n",
    "\n",
    "        json_file = \"C:/Users/arisa/StackOverflow-Project/duplicatetag/{}.json\".format(tag)\n",
    "        #json_file = tag + \".json\"\n",
    "        print(\"dump {} ...\".format(json_file))\n",
    "        with open(json_file, 'w') as json_f:\n",
    "            json.dump(res, json_f, indent=4)\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2816b663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
